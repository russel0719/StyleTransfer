{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c846c1",
   "metadata": {},
   "source": [
    "0. Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f0c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22d2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imageloader import load_images\n",
    "from utils.save_image import save_image\n",
    "from utils.dataloader import load_data\n",
    "from utils.normalize import batch_normalize\n",
    "from utils.gram_matrix import gram_matrix\n",
    "from model.VGG16 import VGG16\n",
    "from model.TransformerNet import TransformerNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3d0d2",
   "metadata": {},
   "source": [
    "1. Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d4b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epoch = 20\n",
    "learning_rate = 1e-4\n",
    "content_weight = 1e5\n",
    "style_weight = 1e10\n",
    "log_interval = 100\n",
    "ckpt_dir = './checkpoints'\n",
    "season1 = 'summer'\n",
    "season2 = 'winter'\n",
    "method = 'shake_feature' # 'one_season', 'shake_data', 'shake_feature'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e38ef",
   "metadata": {},
   "source": [
    "2. Style Images and Train Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a580c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "4000 1000\n"
     ]
    }
   ],
   "source": [
    "if(method == 'one_season' and season1 == season2):\n",
    "    season = season1\n",
    "    style_data = load_images('./data/', season)\n",
    "elif(method == 'shake_data' and season1 != season2):\n",
    "    season = season1 + '_' + season2 + '_data'\n",
    "    style_data1 = load_images('./data/', season1)\n",
    "    style_data2 = load_images('./data/', season2)\n",
    "    idx1 = np.random.choice(10, 5)\n",
    "    idx2 = np.random.choice(10, 5)\n",
    "    style_data = torch.stack([style_data1[i] for i in idx1] + [style_data2[i] for i in idx2],0)\n",
    "elif(method == 'shake_feature' and season1 != season2):\n",
    "    season = season1 + '_' + season2 + '_feature'\n",
    "    style_data1 = load_images('./data/', season1)\n",
    "    style_data2 = load_images('./data/', season2)\n",
    "    style_data = style_data1\n",
    "else:\n",
    "    season = season1\n",
    "    style_data = load_images('./data/', season)\n",
    "print(style_data.shape)\n",
    "\n",
    "train_dataset, train_dataloader, val_dataset, val_dataloader = load_data('./data/', batch_size)\n",
    "print(train_dataset[0][0].shape)\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab84e4",
   "metadata": {},
   "source": [
    "3. Style Transform with gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b7c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb9175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeongmin\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jeongmin\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerNet()\n",
    "vgg = VGG16(requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674484ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(method == 'one_season' and season1 == season2):\n",
    "    features_style = vgg(batch_normalize(style_data.to(device)))\n",
    "    gram_style = [gram_matrix(y) for y in features_style]\n",
    "elif(method == 'shake_data' and season1 != season2):\n",
    "    features_style = vgg(batch_normalize(style_data.to(device)))\n",
    "    gram_style = [gram_matrix(y) for y in features_style]\n",
    "elif(method == 'shake_feature' and season1 != season2):\n",
    "    features_style1 = vgg(batch_normalize(style_data1.to(device)))\n",
    "    features_style2 = vgg(batch_normalize(style_data2.to(device)))\n",
    "    gram_style = [gram_matrix((y1+y2)/2) for y1, y2 in zip(features_style1, features_style2)]\n",
    "else:\n",
    "    features_style = vgg(batch_normalize(style_data.to(device)))\n",
    "    gram_style = [gram_matrix(y) for y in features_style]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeebe31",
   "metadata": {},
   "source": [
    "4. TransformerNet training with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b6b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "929fa48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    best_val_epoch = 0\n",
    "    best_val_loss = math.inf\n",
    "    for epoch in range(num_epoch):\n",
    "        transformer.to(device)\n",
    "        transformer.train()\n",
    "        train_content_loss = 0.\n",
    "        train_style_loss = 0.\n",
    "        count = 0\n",
    "\n",
    "        for batch_id, (x, _) in enumerate(train_dataloader):\n",
    "            n_batch = len(x)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = transformer(x)\n",
    "\n",
    "            y = batch_normalize(y)\n",
    "            x = batch_normalize(x)\n",
    "\n",
    "            features_y = vgg(y)\n",
    "            features_x = vgg(x)\n",
    "\n",
    "            content_loss = content_weight * loss_function(features_y.relu2_2, features_x.relu2_2)\n",
    "\n",
    "            style_loss = 0.\n",
    "            for ft_y, gm_s in zip(features_y, gram_style):\n",
    "                gm_y = gram_matrix(ft_y)\n",
    "                style_loss += loss_function(gm_y, gm_s[:n_batch, :, :])\n",
    "            style_loss *= style_weight\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_content_loss += content_loss.item()\n",
    "            train_style_loss += style_loss.item()\n",
    "\n",
    "            if (batch_id + 1) % log_interval == 0:\n",
    "                msg = \"{}\\tEpoch {}:[{}/{}]\\ttrain\\t[content: {:.4f}\\tstyle: {:.4f}\\ttotal: {:.4f}]\".format(\n",
    "                    time.ctime(), epoch + 1, count, len(train_dataset),\n",
    "                    train_content_loss / (batch_id + 1),\n",
    "                    train_style_loss / (batch_id + 1),\n",
    "                    (train_content_loss + train_style_loss) / (batch_id + 1)\n",
    "                )\n",
    "                print(msg)\n",
    "        # 4.1 Cross-Validataion        \n",
    "        val_content_loss = 0.\n",
    "        val_style_loss = 0.\n",
    "\n",
    "        for (x, _) in val_dataloader:\n",
    "            n_batch = len(x)\n",
    "            x = x.to(device)\n",
    "            y = transformer(x)\n",
    "\n",
    "            y = batch_normalize(y)\n",
    "            x = batch_normalize(x)\n",
    "\n",
    "            features_y = vgg(y)\n",
    "            features_x = vgg(x)\n",
    "\n",
    "            content_loss = content_weight * loss_function(features_y.relu2_2, features_x.relu2_2)\n",
    "\n",
    "            style_loss = 0.\n",
    "            for ft_y, gm_s in zip(features_y, gram_style):\n",
    "                gm_y = gram_matrix(ft_y)\n",
    "                style_loss += loss_function(gm_y, gm_s[:n_batch, :, :])\n",
    "            style_loss *= style_weight\n",
    "\n",
    "            val_content_loss += content_loss.item()\n",
    "            val_style_loss += style_loss.item()\n",
    "\n",
    "        msg = \"\\t\\t\\t\\t\\t\\t\\tval\\t[content: {:.4f}\\tstyle: {:.4f}\\ttotal: {:.4f}]\".format(\n",
    "            val_content_loss / len(val_dataset),\n",
    "            val_style_loss / len(val_dataset),\n",
    "            (val_content_loss + val_style_loss) / len(val_dataset)\n",
    "        )\n",
    "        print(msg)\n",
    "        \n",
    "        train_history.append((train_content_loss + train_style_loss) / len(train_dataset))\n",
    "        val_history.append((val_content_loss + val_style_loss) / len(val_dataset))\n",
    "        \n",
    "        # 4.1 Save Model\n",
    "        if ((val_content_loss + val_style_loss) / len(val_dataset) < best_val_loss):\n",
    "            best_val_epoch = epoch + 1\n",
    "            best_val_loss = (val_content_loss + val_style_loss) / len(val_dataset)\n",
    "            transformer.eval().cpu()\n",
    "            ckpt_model_filename = \"ckpt_epoch_\" + str(epoch + 1) + \"_\" + season + \".pth\"\n",
    "            print(str(epoch + 1), \"th checkpoint is saved!\")\n",
    "            ckpt_model_path = os.path.join(ckpt_dir, ckpt_model_filename)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': transformer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'content_loss': val_content_loss,\n",
    "                'style_loss': val_style_loss,\n",
    "                'total_loss': best_val_loss\n",
    "            }, ckpt_model_path)\n",
    "\n",
    "            transformer.to(device).train()\n",
    "    return train_history, val_history, best_val_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420656f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 12 09:23:25 2022\tEpoch 1:[800/4000]\ttrain\t[content: 514184.0441\tstyle: 2440622.9388\ttotal: 2954806.9828]\n",
      "Mon Dec 12 09:23:59 2022\tEpoch 1:[1600/4000]\ttrain\t[content: 473310.4773\tstyle: 2043350.1413\ttotal: 2516660.6186]\n",
      "Mon Dec 12 09:24:33 2022\tEpoch 1:[2400/4000]\ttrain\t[content: 450540.5759\tstyle: 1773016.4404\ttotal: 2223557.0164]\n",
      "Mon Dec 12 09:25:06 2022\tEpoch 1:[3200/4000]\ttrain\t[content: 434109.4330\tstyle: 1581254.6487\ttotal: 2015364.0817]\n",
      "Mon Dec 12 09:25:40 2022\tEpoch 1:[4000/4000]\ttrain\t[content: 422948.4431\tstyle: 1441845.1325\ttotal: 1864793.5756]\n",
      "\t\t\t\t\t\t\tval\t[content: 47464.6783\tstyle: 103800.8782\ttotal: 151265.5566]\n",
      "1 th checkpoint is saved!\n",
      "Mon Dec 12 09:26:37 2022\tEpoch 2:[800/4000]\ttrain\t[content: 369636.2537\tstyle: 804109.9981\ttotal: 1173746.2519]\n",
      "Mon Dec 12 09:27:11 2022\tEpoch 2:[1600/4000]\ttrain\t[content: 360268.4334\tstyle: 782367.9409\ttotal: 1142636.3744]\n",
      "Mon Dec 12 09:27:44 2022\tEpoch 2:[2400/4000]\ttrain\t[content: 352414.4206\tstyle: 763768.0077\ttotal: 1116182.4283]\n",
      "Mon Dec 12 09:28:18 2022\tEpoch 2:[3200/4000]\ttrain\t[content: 346104.8518\tstyle: 746920.5513\ttotal: 1093025.4031]\n",
      "Mon Dec 12 09:28:51 2022\tEpoch 2:[4000/4000]\ttrain\t[content: 341852.5505\tstyle: 733581.4298\ttotal: 1075433.9803]\n",
      "\t\t\t\t\t\t\tval\t[content: 40761.3968\tstyle: 83388.9721\ttotal: 124150.3688]\n",
      "2 th checkpoint is saved!\n",
      "Mon Dec 12 09:29:49 2022\tEpoch 3:[800/4000]\ttrain\t[content: 318753.3950\tstyle: 665024.2606\ttotal: 983777.6556]\n",
      "Mon Dec 12 09:30:22 2022\tEpoch 3:[1600/4000]\ttrain\t[content: 311765.5807\tstyle: 661022.7716\ttotal: 972788.3523]\n",
      "Mon Dec 12 09:30:56 2022\tEpoch 3:[2400/4000]\ttrain\t[content: 306256.5967\tstyle: 655982.5985\ttotal: 962239.1952]\n",
      "Mon Dec 12 09:31:29 2022\tEpoch 3:[3200/4000]\ttrain\t[content: 302420.7304\tstyle: 649834.0752\ttotal: 952254.8056]\n",
      "Mon Dec 12 09:32:03 2022\tEpoch 3:[4000/4000]\ttrain\t[content: 300569.9148\tstyle: 645177.6173\ttotal: 945747.5321]\n",
      "\t\t\t\t\t\t\tval\t[content: 36936.9520\tstyle: 77355.8807\ttotal: 114292.8327]\n",
      "3 th checkpoint is saved!\n",
      "Mon Dec 12 09:33:00 2022\tEpoch 4:[800/4000]\ttrain\t[content: 290239.3864\tstyle: 620631.9175\ttotal: 910871.3039]\n",
      "Mon Dec 12 09:33:34 2022\tEpoch 4:[1600/4000]\ttrain\t[content: 285426.9121\tstyle: 618926.7597\ttotal: 904353.6718]\n",
      "Mon Dec 12 09:34:08 2022\tEpoch 4:[2400/4000]\ttrain\t[content: 281911.6242\tstyle: 616130.9775\ttotal: 898042.6017]\n",
      "Mon Dec 12 09:34:41 2022\tEpoch 4:[3200/4000]\ttrain\t[content: 279634.6964\tstyle: 612621.6678\ttotal: 892256.3643]\n",
      "Mon Dec 12 09:35:15 2022\tEpoch 4:[4000/4000]\ttrain\t[content: 279137.4195\tstyle: 609910.0856\ttotal: 889047.5052]\n",
      "\t\t\t\t\t\t\tval\t[content: 35075.2907\tstyle: 74066.3565\ttotal: 109141.6472]\n",
      "4 th checkpoint is saved!\n",
      "Mon Dec 12 09:36:12 2022\tEpoch 5:[800/4000]\ttrain\t[content: 275859.2705\tstyle: 596487.1981\ttotal: 872346.4686]\n",
      "Mon Dec 12 09:36:46 2022\tEpoch 5:[1600/4000]\ttrain\t[content: 272340.8803\tstyle: 594956.4883\ttotal: 867297.3686]\n",
      "Mon Dec 12 09:37:19 2022\tEpoch 5:[2400/4000]\ttrain\t[content: 269753.0056\tstyle: 593123.9961\ttotal: 862877.0018]\n",
      "Mon Dec 12 09:37:53 2022\tEpoch 5:[3200/4000]\ttrain\t[content: 268178.2082\tstyle: 590891.5702\ttotal: 859069.7785]\n",
      "Mon Dec 12 09:38:26 2022\tEpoch 5:[4000/4000]\ttrain\t[content: 268292.0564\tstyle: 589159.2853\ttotal: 857451.3417]\n",
      "\t\t\t\t\t\t\tval\t[content: 34254.4422\tstyle: 71933.3008\ttotal: 106187.7430]\n",
      "5 th checkpoint is saved!\n",
      "Mon Dec 12 09:39:24 2022\tEpoch 6:[800/4000]\ttrain\t[content: 268621.6966\tstyle: 580538.1119\ttotal: 849159.8084]\n",
      "Mon Dec 12 09:40:01 2022\tEpoch 6:[1600/4000]\ttrain\t[content: 265719.2722\tstyle: 579260.1527\ttotal: 844979.4248]\n",
      "Mon Dec 12 09:40:39 2022\tEpoch 6:[2400/4000]\ttrain\t[content: 263587.5324\tstyle: 577893.5266\ttotal: 841481.0590]\n",
      "Mon Dec 12 09:41:17 2022\tEpoch 6:[3200/4000]\ttrain\t[content: 262279.9654\tstyle: 576488.5556\ttotal: 838768.5211]\n",
      "Mon Dec 12 09:41:55 2022\tEpoch 6:[4000/4000]\ttrain\t[content: 262679.6153\tstyle: 575415.4931\ttotal: 838095.1084]\n",
      "\t\t\t\t\t\t\tval\t[content: 33780.6992\tstyle: 70592.2251\ttotal: 104372.9242]\n",
      "6 th checkpoint is saved!\n",
      "Mon Dec 12 09:42:59 2022\tEpoch 7:[800/4000]\ttrain\t[content: 264624.8889\tstyle: 569889.9494\ttotal: 834514.8383]\n",
      "Mon Dec 12 09:43:37 2022\tEpoch 7:[1600/4000]\ttrain\t[content: 261844.2948\tstyle: 569306.2300\ttotal: 831150.5248]\n",
      "Mon Dec 12 09:44:15 2022\tEpoch 7:[2400/4000]\ttrain\t[content: 259842.0140\tstyle: 568345.6622\ttotal: 828187.6761]\n",
      "Mon Dec 12 09:44:53 2022\tEpoch 7:[3200/4000]\ttrain\t[content: 258691.4864\tstyle: 567394.6659\ttotal: 826086.1523]\n",
      "Mon Dec 12 09:45:31 2022\tEpoch 7:[4000/4000]\ttrain\t[content: 259208.6269\tstyle: 566675.6546\ttotal: 825884.2816]\n",
      "\t\t\t\t\t\t\tval\t[content: 33416.3617\tstyle: 69827.8515\ttotal: 103244.2132]\n",
      "7 th checkpoint is saved!\n",
      "Mon Dec 12 09:46:35 2022\tEpoch 8:[800/4000]\ttrain\t[content: 261859.9572\tstyle: 562995.5828\ttotal: 824855.5400]\n",
      "Mon Dec 12 09:47:13 2022\tEpoch 8:[1600/4000]\ttrain\t[content: 259140.7491\tstyle: 562719.4772\ttotal: 821860.2262]\n",
      "Mon Dec 12 09:47:51 2022\tEpoch 8:[2400/4000]\ttrain\t[content: 257182.7071\tstyle: 562042.9070\ttotal: 819225.6141]\n",
      "Mon Dec 12 09:48:29 2022\tEpoch 8:[3200/4000]\ttrain\t[content: 256114.5988\tstyle: 561355.8199\ttotal: 817470.4187]\n",
      "Mon Dec 12 09:49:07 2022\tEpoch 8:[4000/4000]\ttrain\t[content: 256707.7164\tstyle: 560850.5943\ttotal: 817558.3107]\n",
      "\t\t\t\t\t\t\tval\t[content: 33118.2278\tstyle: 69294.1873\ttotal: 102412.4151]\n",
      "8 th checkpoint is saved!\n",
      "Mon Dec 12 09:50:11 2022\tEpoch 9:[800/4000]\ttrain\t[content: 259801.3942\tstyle: 558070.0556\ttotal: 817871.4498]\n",
      "Mon Dec 12 09:50:49 2022\tEpoch 9:[1600/4000]\ttrain\t[content: 257191.2814\tstyle: 557981.4094\ttotal: 815172.6908]\n",
      "Mon Dec 12 09:51:27 2022\tEpoch 9:[2400/4000]\ttrain\t[content: 255224.0630\tstyle: 557409.9516\ttotal: 812634.0146]\n",
      "Mon Dec 12 09:52:06 2022\tEpoch 9:[3200/4000]\ttrain\t[content: 254232.2927\tstyle: 556858.2905\ttotal: 811090.5831]\n",
      "Mon Dec 12 09:52:44 2022\tEpoch 9:[4000/4000]\ttrain\t[content: 254862.4327\tstyle: 556459.7413\ttotal: 811322.1740]\n",
      "\t\t\t\t\t\t\tval\t[content: 32854.4421\tstyle: 68923.0668\ttotal: 101777.5089]\n",
      "9 th checkpoint is saved!\n",
      "Mon Dec 12 09:53:48 2022\tEpoch 10:[800/4000]\ttrain\t[content: 258061.0305\tstyle: 554301.3806\ttotal: 812362.4111]\n",
      "Mon Dec 12 09:54:26 2022\tEpoch 10:[1600/4000]\ttrain\t[content: 255555.3714\tstyle: 554208.2417\ttotal: 809763.6131]\n",
      "Mon Dec 12 09:55:04 2022\tEpoch 10:[2400/4000]\ttrain\t[content: 253599.5014\tstyle: 553658.4880\ttotal: 807257.9894]\n",
      "Mon Dec 12 09:55:42 2022\tEpoch 10:[3200/4000]\ttrain\t[content: 252670.6002\tstyle: 553194.1942\ttotal: 805864.7944]\n",
      "Mon Dec 12 09:56:20 2022\tEpoch 10:[4000/4000]\ttrain\t[content: 253330.8254\tstyle: 552863.9969\ttotal: 806194.8222]\n",
      "\t\t\t\t\t\t\tval\t[content: 32663.1278\tstyle: 68594.7202\ttotal: 101257.8480]\n",
      "10 th checkpoint is saved!\n",
      "Mon Dec 12 09:57:25 2022\tEpoch 11:[800/4000]\ttrain\t[content: 256659.3513\tstyle: 551049.3178\ttotal: 807708.6691]\n",
      "Mon Dec 12 09:58:03 2022\tEpoch 11:[1600/4000]\ttrain\t[content: 254232.3653\tstyle: 551015.7444\ttotal: 805248.1097]\n",
      "Mon Dec 12 09:58:39 2022\tEpoch 11:[2400/4000]\ttrain\t[content: 252298.6532\tstyle: 550539.0461\ttotal: 802837.6994]\n",
      "Mon Dec 12 09:59:13 2022\tEpoch 11:[3200/4000]\ttrain\t[content: 251410.2022\tstyle: 550117.7452\ttotal: 801527.9474]\n",
      "Mon Dec 12 09:59:48 2022\tEpoch 11:[4000/4000]\ttrain\t[content: 252079.5770\tstyle: 549790.3721\ttotal: 801869.9492]\n",
      "\t\t\t\t\t\t\tval\t[content: 32508.4179\tstyle: 68286.0115\ttotal: 100794.4293]\n",
      "11 th checkpoint is saved!\n",
      "Mon Dec 12 10:00:47 2022\tEpoch 12:[800/4000]\ttrain\t[content: 255391.0847\tstyle: 548165.8203\ttotal: 803556.9050]\n"
     ]
    }
   ],
   "source": [
    "train_history, val_history, best_val_epoch = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5e5d1",
   "metadata": {},
   "source": [
    "5. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d668d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    content_data = load_images('./data/', 'test')\n",
    "    content_data = content_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        style_model = TransformerNet()\n",
    "        \n",
    "        ckpt_model_path = os.path.join(ckpt_dir, f'ckpt_epoch_{best_val_epoch}_{season}.pth')\n",
    "        checkpoint = torch.load(ckpt_model_path, map_location=device)\n",
    "        \n",
    "        # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "        for k in list(checkpoint.keys()):\n",
    "            if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "                del checkpoint[k]\n",
    "        \n",
    "        style_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        style_model.to(device)\n",
    "        \n",
    "        output = style_model(content_data).cpu()\n",
    "        save_image(f'./outputs/output_{season}.png', output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f672e",
   "metadata": {},
   "source": [
    "6. Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_history, val_history):\n",
    "    x = range(1, num_epoch + 1)\n",
    "    plt.plot(x, train_history, color='red', label='train loss')\n",
    "    plt.plot(x, val_history, color='blue', label='val loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(f'./outputs/loss_graph_{season}.png')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_history, val_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
