{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c846c1",
   "metadata": {},
   "source": [
    "0. Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imageloader import load_images\n",
    "from utils.save_image import save_image\n",
    "from utils.dataloader import load_data\n",
    "from utils.normalize import batch_normalize\n",
    "from utils.gram_matrix import gram_matrix\n",
    "from model.VGG16 import VGG16\n",
    "from model.TransformerNet import TransformerNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3d0d2",
   "metadata": {},
   "source": [
    "1. Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epoch = 60\n",
    "learning_rate = 1e-4\n",
    "content_weight = 1e5\n",
    "style_weight = 1e10\n",
    "log_interval = 50\n",
    "ckpt_dir = './checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e38ef",
   "metadata": {},
   "source": [
    "2. Style Images and Train Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a580c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_data = load_images('./data/', 'summer', batch_size)\n",
    "print(style_data.shape)\n",
    "\n",
    "train_dataset, train_dataloader, val_dataset, val_dataloader = load_data('./data/', batch_size)\n",
    "print(train_dataset[0][0].shape)\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab84e4",
   "metadata": {},
   "source": [
    "3. Style Transform with gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b7c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerNet().to(device)\n",
    "vgg = VGG16(requires_grad=False).to(device)\n",
    "\n",
    "features_style = vgg(batch_normalize(style_data.to(device)))\n",
    "gram_style = [gram_matrix(y) for y in features_style]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeebe31",
   "metadata": {},
   "source": [
    "4. TransformerNet training with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b6b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(transformer.parameters(), lr=learning_rate)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929fa48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(num_epoch):\n",
    "        transformer.train()\n",
    "        train_content_loss = 0.\n",
    "        train_style_loss = 0.\n",
    "        count = 0\n",
    "\n",
    "        for batch_id, (x, _) in enumerate(train_dataloader):\n",
    "            n_batch = len(x)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = transformer(x)\n",
    "\n",
    "            y = batch_normalize(y)\n",
    "            x = batch_normalize(x)\n",
    "\n",
    "            features_y = vgg(y)\n",
    "            features_x = vgg(x)\n",
    "\n",
    "            content_loss = content_weight * loss_function(features_y.relu2_2, features_x.relu2_2)\n",
    "\n",
    "            style_loss = 0.\n",
    "            for ft_y, gm_s in zip(features_y, gram_style):\n",
    "                gm_y = gram_matrix(ft_y)\n",
    "                style_loss += loss_function(gm_y, gm_s[:n_batch, :, :])\n",
    "            style_loss *= style_weight\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_content_loss += content_loss.item()\n",
    "            train_style_loss += style_loss.item()\n",
    "\n",
    "            if (batch_id + 1) % log_interval == 0:\n",
    "                val_content_loss = 0.\n",
    "                val_style_loss = 0.\n",
    "                \n",
    "                for (x, _) in val_dataloader:\n",
    "                    n_batch = len(x)\n",
    "                    x = x.to(device)\n",
    "                    y = transformer(x)\n",
    "\n",
    "                    y = batch_normalize(y)\n",
    "                    x = batch_normalize(x)\n",
    "\n",
    "                    features_y = vgg(y)\n",
    "                    features_x = vgg(x)\n",
    "                    \n",
    "                    content_loss = content_weight * loss_function(features_y.relu2_2, features_x.relu2_2)\n",
    "\n",
    "                    style_loss = 0.\n",
    "                    for ft_y, gm_s in zip(features_y, gram_style):\n",
    "                        gm_y = gram_matrix(ft_y)\n",
    "                        style_loss += loss_function(gm_y, gm_s[:n_batch, :, :])\n",
    "                    style_loss *= style_weight\n",
    "                    \n",
    "                    val_content_loss += content_loss.item()\n",
    "                    val_style_loss += style_loss.item()\n",
    "                \n",
    "                msg = \"{}\\tEpoch {}:[{}/{}]\\ttrain\\t[content: {:.4f}\\tstyle: {:.4f}\\ttotal: {:.4f}]\".format(\n",
    "                    time.ctime(), epoch + 1, count, len(train_dataset),\n",
    "                    train_content_loss / (batch_id + 1),\n",
    "                    train_style_loss / (batch_id + 1),\n",
    "                    (train_content_loss + train_style_loss) / (batch_id + 1)\n",
    "                )\n",
    "                print(msg)\n",
    "                msg = \"\\t\\t\\t\\t\\t\\t\\tval\\t[content: {:.4f}\\tstyle: {:.4f}\\ttotal: {:.4f}]\".format(\n",
    "                    val_content_loss / len(val_dataset),\n",
    "                    val_style_loss / len(val_dataset),\n",
    "                    (val_content_loss + val_style_loss) / len(val_dataset)\n",
    "                )\n",
    "                print(msg)\n",
    "                \n",
    "        # 4.1 Save Model\n",
    "        transformer.eval().cpu()\n",
    "        ckpt_model_filename = \"ckpt_epoch_\" + str(epoch + 1) + \"_batch_id_\" + str(batch_id + 1) + \".pth\"\n",
    "        print(str(epoch + 1), \"th checkpoint is saved!\")\n",
    "        ckpt_model_path = os.path.join(ckpt_dir, ckpt_model_filename)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': transformer.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': total_loss\n",
    "        }, ckpt_model_path)\n",
    "\n",
    "        transformer.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420656f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5e5d1",
   "metadata": {},
   "source": [
    "5. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d668d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    content_data = load_images('./data/', 'test', 1)\n",
    "    content_data = content_data.to(device)\n",
    "    with torch.no_grad():\n",
    "        style_model = TransformerNet()\n",
    "        \n",
    "        ckpt_model_path = os.path.join(ckpt_dir, \"ckpt_epoch_30_batch_id_280.pth\") #FIXME\n",
    "        checkpoint = torch.load(ckpt_model_path, map_location=device)\n",
    "        \n",
    "        # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "        for k in list(checkpoint.keys()):\n",
    "            if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
    "                del checkpoint[k]\n",
    "        \n",
    "        style_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        style_model.to(device)\n",
    "        \n",
    "        output = style_model(content_data).cpu()\n",
    "        save_image('./output.png', output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
